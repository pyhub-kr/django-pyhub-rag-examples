import json
import os
import sqlite3
from dataclasses import dataclass
from decimal import Decimal
from pathlib import Path
from typing import Generator, List, Literal, TypeAlias, Union

import openai
import sqlite_vec
import streamlit as st
from dotenv import load_dotenv
from openai.types import ChatModel as OpenAIChatModel
from typing_extensions import Optional

OpenAIEmbeddingModel: TypeAlias = Union[
    Literal[
        "text-embedding-3-small",  # 1536 ì°¨ì›
        "text-embedding-3-large",  # 3072 ì°¨ì›
    ],
    str,
]

LLMChatModel: TypeAlias = Union[str, OpenAIChatModel]


# https://platform.openai.com/docs/pricing#embeddings
def get_embedding_price(
    model: OpenAIEmbeddingModel, tokens: int
) -> tuple[Decimal, Decimal]:
    # 2025ë…„ 3ì›” ê¸°ì¤€
    price_per_1m = {
        "text-embedding-3-small": Decimal("0.02"),
        "text-embedding-3-large": Decimal("0.13"),
    }[model]
    usd = (Decimal(tokens) * price_per_1m) / Decimal("1000000")
    krw = usd * Decimal("1500")
    return usd, krw


# https://platform.openai.com/docs/pricing#latest-models
def get_chat_price(
    model: LLMChatModel, input_tokens: int, output_tokens: int
) -> tuple[Decimal, Decimal]:
    input_price_per_1m, output_price_per_1m = {
        "gpt-4o": (Decimal("2.5"), Decimal("10.0")),
        "gpt-4o-mini": (Decimal("0.15"), Decimal("0.60")),
        "o1": (Decimal("15"), Decimal("60.00")),
        "o3-mini": (Decimal("1.10"), Decimal("4.40")),
        "o1-mini": (Decimal("1.10"), Decimal("4.40")),
    }[model]

    input_usd = (Decimal(input_tokens) * input_price_per_1m) / Decimal("1000000")
    output_usd = (Decimal(output_tokens) * output_price_per_1m) / Decimal("1000000")
    usd = input_usd + output_usd

    input_krw = input_usd * Decimal("1500")
    output_krw = output_usd * Decimal("1500")
    krw = input_krw + output_krw

    return usd, krw


st.set_page_config(
    page_title="simple rag with streamlit (Powered by django-pyhub-rag)",
    page_icon="ğŸ”",
    layout="wide",
)

st.title("ğŸ“š ì„¸ë²• í•´ì„ë¡€ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰")
st.markdown(
    """
    íŒŒì´ì¬ì‚¬ë‘ë°© [ì¥ê³ ë¡œ ë§Œë“œëŠ” RAG ì›¹ ì±„íŒ… ì„œë¹„ìŠ¤](https://ai.pyhub.kr/hands-on-lab/django-webchat-rag/) íŠœí† ë¦¬ì–¼ì„ í†µí•´
    ìƒì„±ëœ sqlite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    (ì°¸ê³ : [êµ­ì„¸ë²•ë ¹ì •ë³´ì‹œìŠ¤í…œ](https://taxlaw.nts.go.kr/)ì—ëŠ”
    [13ë§Œ ê±´ì´ ë„˜ëŠ” ì„¸ë²•í•´ì„ë¡€ ì§ˆë‹µ ë°ì´í„°](https://taxlaw.nts.go.kr/qt/USEQTJ001M.do)ê°€ ìˆìŠµë‹ˆë‹¤.)
    """
)


@dataclass
class Document:
    id: int
    page_content: str
    metadata: dict
    distance: float

    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì‹¤í–‰ë˜ëŠ” ë©”ì„œë“œë¡œ í•„ìš”í•œ ì†ì„±ë“¤ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        # page_contentì—ì„œ í•„ìš”í•œ ì†ì„±ë“¤ì„ íŒŒì‹±í•˜ì—¬ í• ë‹¹
        obj = json.loads(self.page_content)
        self.ë¬¸ì„œID = obj["ë¬¸ì„œID"]
        self.ì œëª© = obj["ì œëª©"]
        self.ë¬¸ì„œë²ˆí˜¸ = obj["ë¬¸ì„œë²ˆí˜¸"]
        self.ë²•ë ¹ë¶„ë¥˜ = obj["ë²•ë ¹ë¶„ë¥˜"]
        self.ìš”ì§€ = obj["ìš”ì§€"]
        self.íšŒì‹  = obj["íšŒì‹ "]
        self.íŒŒì¼ë‚´ìš© = obj["íŒŒì¼ë‚´ìš©"]
        self.ê³µê°œì—¬ë¶€ = obj["ê³µê°œì—¬ë¶€"]
        self.ë¬¸ì„œë¶„ë¥˜ = obj["ë¬¸ì„œë¶„ë¥˜"]
        self.ìƒì„±ì¼ì‹œ = obj["ìƒì„±ì¼ì‹œ"]
        self.ìˆ˜ì •ì¼ì‹œ = obj["ìˆ˜ì •ì¼ì‹œ"]
        self.url = self.metadata.get("url", None)


class Rag:
    def __init__(
        self,
        db_path: str,
        table_name: str,
        embedding_model: OpenAIEmbeddingModel,
        chat_model: LLMChatModel,
        openai_api_key: Optional[str] = None,
    ):
        self.db_path = db_path
        self.table_name = table_name
        self.embedding_model = embedding_model
        self.chat_model = chat_model
        self.openai_api_key = openai_api_key

    def embed(self, input: str) -> tuple[list[float], int]:
        client = openai.Client(api_key=self.openai_api_key)
        response = client.embeddings.create(
            input=input,
            model=self.embedding_model,
        )
        return response.data[0].embedding, response.usage.prompt_tokens

    def make_reply(
        self,
        system_prompt: Optional[str],
        user_prompt: str,
    ) -> Generator[str, None, None]:
        text_output = ""

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})

        response = openai.chat.completions.create(
            model=self.chat_model,
            messages=messages,
            stream=True,
            stream_options={"include_usage": True},
        )
        # ìƒì„± ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ ëˆ„ì  ì²˜ë¦¬
        usage = None
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                text_output += chunk.choices[0].delta.content
                yield text_output

            if chunk.usage:
                usage = chunk.usage

        if usage:
            input_tokens = usage.prompt_tokens
            output_tokens = usage.completion_tokens
            usd, krw = get_chat_price(self.chat_model, input_tokens, output_tokens)
            text_output += f"\n\n(ì…ë ¥ í† í°: {input_tokens}, ì¶œë ¥ í† í°: {output_tokens}, ë¹„ìš©: ${usd}, â‚©{krw})"
            yield text_output

    def similarity_search(
        self, embedding_vector: list[float], k: int = 4
    ) -> List[Document]:
        with sqlite3.connect(self.db_path) as conn:
            conn.enable_load_extension(True)
            sqlite_vec.load(conn)
            conn.enable_load_extension(False)

            cursor = conn.cursor()
            cursor.execute(
                f"""
                SELECT id, page_content, metadata, distance
                FROM {self.table_name}
                WHERE (embedding MATCH vec_f32(?))
                ORDER BY distance
                LIMIT {k}
                """,
                [str(embedding_vector)],
            )

            results = []
            for row in cursor.fetchall():
                results.append(
                    Document(
                        id=row[0],
                        page_content=row[1],
                        metadata=json.loads(row[2]),
                        distance=row[3],
                    )
                )

        return results


def main(
    db_path: Union[Path, str],
    table_name: str,
    embedding_model: OpenAIEmbeddingModel,
    chat_model: LLMChatModel,
    system_prompt: str,
    openai_api_key: Optional[str] = None,
):
    rag = Rag(
        db_path=db_path,
        table_name=table_name,
        embedding_model=embedding_model,
        chat_model=chat_model,
        openai_api_key=openai_api_key,
    )

    default_query = (
        "ì¬í™” ìˆ˜ì¶œí•˜ëŠ” ê²½ìš° ì˜ì„¸ìœ¨ ì²¨ë¶€ ì„œë¥˜ë¡œ ìˆ˜ì¶œì‹¤ì ëª…ì„¸ì„œê°€ ì—†ëŠ” ê²½ìš° í•´ê²° ë°©ë²•"
    )

    # Search interface
    with st.container():
        search_query = st.text_input(
            "ğŸ” ê²€ìƒ‰ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            placeholder=default_query,
        ).strip()
        search_button = st.button("ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°")

    if search_button:
        if not search_query:
            search_query = default_query

        st.markdown(f"### ìœ ì‚¬ ë¬¸ì„œ")

        with st.spinner("ì°¾ëŠ” ì¤‘ ..."):
            embedding_vector, tokens = rag.embed(search_query)
            usd_price, krw_price = get_embedding_price(rag.embedding_model, tokens)
            doc_list = rag.similarity_search(embedding_vector)

        st.markdown(
            f"{len(doc_list)} ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. (í† í°: {tokens}, ì„ë² ë”© ë¹„ìš©: ${usd_price}, â‚©{krw_price})"
        )

        if doc_list:
            for doc in doc_list:
                with st.expander(f"{doc.ì œëª©} (ìœ ì‚¬ë„ ê±°ë¦¬ : {doc.distance:.4f})"):
                    meta_data = {
                        "ë¬¸ì„œID": doc.ë¬¸ì„œID,
                        "ê³µê°œì—¬ë¶€": doc.ê³µê°œì—¬ë¶€,
                        "ë¬¸ì„œë¶„ë¥˜": doc.ë¬¸ì„œë¶„ë¥˜,
                        "ë²•ë ¹ë¶„ë¥˜": doc.ë²•ë ¹ë¶„ë¥˜,
                        "ìƒì„±ì¼ì‹œ": doc.ìƒì„±ì¼ì‹œ,
                        "ìˆ˜ì •ì¼ì‹œ": doc.ìˆ˜ì •ì¼ì‹œ,
                    }

                    st.markdown("#### ë¬¸ì„œ ë©”íƒ€ë°ì´í„°")
                    st.table(meta_data)

                    st.markdown("#### íŒŒì¼ë‚´ìš©")
                    st.markdown(doc.íŒŒì¼ë‚´ìš©)

                    st.markdown("#### ìš”ì§€")
                    st.markdown(doc.ìš”ì§€)

                    if doc.url:
                        st.markdown(f"**[ì›ë¬¸ ë³´ê¸°]({doc.url})**")
        else:
            st.info("No documents found matching your search criteria.")

        st.markdown(f"### LLM ì‘ë‹µ (ëª¨ë¸: {rag.chat_model})")

        with st.spinner("LLM ì‘ë‹µ ìƒì„± ì¤‘ ..."):
            # ì‘ë‹µì„ í‘œì‹œí•  ë¹ˆ markdown ì»´í¬ë„ŒíŠ¸ ìƒì„±
            response_container = st.empty()

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for current_text in rag.make_reply(
                system_prompt=system_prompt + "\n\n" + f"<context>{doc_list}</context>",
                user_prompt=f"Question: {search_query}",
            ):
                # ëˆ„ì ëœ í…ìŠ¤íŠ¸ë¡œ ì»´í¬ë„ŒíŠ¸ ì—…ë°ì´íŠ¸
                response_container.markdown(current_text)

    # Footer
    st.markdown("---")
    st.markdown(
        """
<div style="text-align: center; color: gray; font-size: 0.8em;">
ë§Œë“ ì´ : íŒŒì´ì¬ì‚¬ë‘ë°© <a href="mailto:me@pyhub.kr">me@pyhub.kr</a> , ìë¬¸ : ì„œì°¬ì˜ì„¸ë¬´íšŒê³„ì‚¬ë¬´ì†Œ
</div>
        """,
        unsafe_allow_html=True,
    )


if __name__ == "__main__":
    load_dotenv()

    try:
        with open("system_prompt.txt", "rt", encoding="utf-8") as f:
            loaded_system_prompt = f.read()
    except IOError:
        loaded_system_prompt = None

    main(
        db_path="./sample-taxlaw-1000.sqlite3",
        table_name="taxlaw_documents",
        embedding_model="text-embedding-3-large",
        chat_model="gpt-4o",
        system_prompt=loaded_system_prompt,
        # .env íŒŒì¼ì—ì„œ ë¡œë“œëœ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )
